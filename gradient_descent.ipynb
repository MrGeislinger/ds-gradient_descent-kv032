{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Solving-the-Line-of-Best-by-Guessing\" data-toc-modified-id=\"Solving-the-Line-of-Best-by-Guessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Solving the Line of Best by Guessing</a></span></li><li><span><a href=\"#Defining-the-Loss-&amp;-Cost-Functions\" data-toc-modified-id=\"Defining-the-Loss-&amp;-Cost-Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Defining the Loss &amp; Cost Functions</a></span></li><li><span><a href=\"#Better-Way-of-Guessing:-Gradient-Descent\" data-toc-modified-id=\"Better-Way-of-Guessing:-Gradient-Descent-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Better Way of Guessing: Gradient Descent</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gradient-Descent-in-Words\" data-toc-modified-id=\"Gradient-Descent-in-Words-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Gradient Descent in Words</a></span></li><li><span><a href=\"#Example\" data-toc-modified-id=\"Example-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Example</a></span></li><li><span><a href=\"#Step-Size\" data-toc-modified-id=\"Step-Size-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Step Size</a></span></li><li><span><a href=\"#Formula\" data-toc-modified-id=\"Formula-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Formula</a></span></li></ul></li><li><span><a href=\"#ðŸ‘‡-PAST-MATERIAL\" data-toc-modified-id=\"ðŸ‘‡-PAST-MATERIAL-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>ðŸ‘‡ PAST MATERIAL</a></span></li><li><span><a href=\"#Exercise\" data-toc-modified-id=\"Exercise-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Exercise</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-It!\" data-toc-modified-id=\"Test-It!-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Test It!</a></span></li></ul></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Exercises</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Explain and use the concept of a gradient\n",
    "- Explain the algorithm of gradient descent\n",
    "- Describe the effect of the \"learning rate\" in the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Solving the Line of Best by Guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Defining the Loss & Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have already encountered the notion of a loss function. For an ordinary least-squares regression, the typical loss function is simply the sum of squared differences between the true $y$-values and the corresponding values on the best-fit line. And the goal, of course, is to **minimize** our loss function. (This is not of course to say that we can make it negligibly small or zero; the nature of the data and the nature of the model will impose restrictions on how much progress we can make.)\n",
    "\n",
    "The fundamental idea we take from calculus here is that **the minimum of the loss function must occur where its slope is zero**. To calculate, analytically, the parameters of the best-fit line, we construct the derivative of the least-squares function with respect to the parameters of the line (slope and y-intercept) and then set the resulting expressions to zero, solving for the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Better Way of Guessing: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Gradient Descent** is an optimization procedure that, in one form or another, underlies many model-fitting algorithms. The gradient, in concept, is a generalized notion of a derivative and so belongs to **calculus** (or **analysis**).\n",
    "\n",
    "The gradient is defined as follows:\n",
    "\n",
    "If $f(x_1, ... , x_n)$ is a function of $n$ variables, then its gradient is equal to:\n",
    "\n",
    "$\\large\\nabla f(x_1, ... , x_n) = \\frac{\\partial f}{\\partial x_1}\\hat{x_1} + ... + \\frac{\\partial f}{\\partial x_n}\\hat{x_n}$,\n",
    "\n",
    "where $\\hat{x_k}$ is the unit vector in the $\\vec{x_k}$-direction.\n",
    "\n",
    "Recall that, in the single-variable case, a positive derivative indicates an increasing function and a negative derivative indicates a decreasing function.\n",
    "\n",
    "In the multivariate case, the gradient tells us how the function is changing **in each dimension**. A large value of the derivative with respect to a particular variable means that the gradient will have a large component in the corresponding direction. Therefore, **the gradient will point in the direction of steepest increase**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose that we have these data points:\n",
    "\n",
    "(1, 2), (3, 9), (5, 10),\n",
    "\n",
    "and that we're interested in constructing a best-fit line through them.\n",
    "\n",
    "Now this optimization problem has a well-known solution and we could simply plug in our values here into that formula.\n",
    "\n",
    "The solution to a linear regression problem is a matter of minimizing the sum of squared distances between actual y-values and the line's predictions.\n",
    "\n",
    "More generally, model optimization involves setting the derivative of the function you want to minimize (for a linear regression, this would be the sum of the squared distances between $y$ and $\\hat{y}$) to zero, and then solving for the parameters that define the model (for a linear regression, this would be the slope and y-intercept).\n",
    "\n",
    "But this is not always easily or efficiently done. Sometimes the loss function is quite complicated and it is not a straightforward matter to set the derivative equal to zero and to solve the resulting equation(s) for the missing parameters.\n",
    "\n",
    "Sometimes it's easier or more efficient to use gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Gradient Descent in Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- make a guess at where the function attains its minimum value; and then\n",
    "- calculate the derivative at that point; and then\n",
    "- use that value to decide how to make your next guess!\n",
    "\n",
    "Repeat until we get the derivative as close as we like to 0.\n",
    "\n",
    "If we want to improve our guess at the minimum of our loss function, we'll move in the **opposite direction** of the gradient away from our last guess. Hence we are using the *gradient* of our loss function to *descend* to the minimum value of the relevant loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try this with our three data points above.\n",
    "\n",
    "You may recall that the parameters that produce the best-fit line for the three points above are:\n",
    "\n",
    "$\\beta_0 = 1$; <br/>\n",
    "$\\beta_1 = 2$.\n",
    "\n",
    "But suppose we try to arrive at these values by guessing and checking, i.e. by (a crude version of) the method of gradient descent.\n",
    "\n",
    "Let's start with a guess of:\n",
    "\n",
    "$\\beta_0 = 3$; <br/>\n",
    "$\\beta_1 = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "beta_0 = 3\n",
    "beta_1 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now the function we're trying to minimize is:\n",
    "\n",
    "$\\large f(\\beta_0, \\beta_1) = \\Sigma(y - \\beta_1x - \\beta_0)^2$.\n",
    "\n",
    "So we have:\n",
    "\n",
    "$\\large\\frac{\\partial f}{\\partial\\beta_0} = -2\\Sigma(y - \\beta_1x - \\beta_0)$; and\n",
    "\n",
    "$\\large\\frac{\\partial f}{\\partial\\beta_1} = -2\\Sigma x(y - \\beta_1x - \\beta_0)$.\n",
    "\n",
    "The answer to the optimization problem is $\\beta_0=1$, $\\beta_1=2$.\n",
    "\n",
    "Note that if we plug in $\\beta_0 = 1$, $\\beta_1 = 2$, we get 0 for these derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dfdbeta0(beta_0, beta_1):\n",
    "    return -2 * ((2 - beta_1*1 - beta_0)\\\n",
    "                 + (9 - beta_1*3 - beta_0)\\\n",
    "                 + (10 - beta_1*5 - beta_0))\n",
    "\n",
    "def dfdbeta1(beta_0, beta_1):\n",
    "    return -2 * ((1 * (2 - beta_1*1 - beta_0))\\\n",
    "                 + (3 * (9 - beta_1*3 - beta_0))\\\n",
    "                 + (5 * (10 - beta_1*5 - beta_0)))\n",
    "\n",
    "partial_0 = dfdbeta0(1, 2)\n",
    "partial_1 = dfdbeta1(1, 2)\n",
    "\n",
    "partial_0, partial_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But let's plug in our guesses and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "partial_0 = dfdbeta0(3, 3)\n",
    "partial_1 = dfdbeta1(3, 3)\n",
    "\n",
    "partial_0, partial_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since $\\frac{\\partial f}{\\partial\\beta_0}$ and $\\frac{\\partial f}{\\partial\\beta_1}$ are positive, this tells us that we need to make our guesses **smaller** for each. So we might try $\\beta_0 = \\beta_1 = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "partial_0 = dfdbeta0(1, 1)\n",
    "partial_1 = dfdbeta1(1, 1)\n",
    "\n",
    "partial_0, partial_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we need to make our guesses **larger**! Note that, even though $\\beta_0 = 1$ is part of the optimal solution, there is no guarantee that the associated partial derivative will be 0 for all the points in parameter space where $\\beta_0 = 1$, since the derivative is a function both of $\\beta_0$ and of $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Step Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How can we guarantee that we don't step **too far** when making a new estimate of the optimal parameter values? If we step too far we may never reach the minimum. And for that matter, there is reason also not to have our step size be **too small**. If we're far from the minimum, then a small learning rate would make our route to the minimum very costly (in terms of computation time, if nothing else).\n",
    "\n",
    "Here's an elegant solution: Make the size of your step **proportional to the value of the derivative at the point where you currently are in parameter space**! If we're very far from the minimum, then our values will be large, and so we therefore can safely take a large step; if we're close to the minimum, then our values will be small, and so we should therefore take a smaller step.\n",
    "\n",
    "I said the size of the step is proportional to the value of the derivative. The constant of proportionality is often called the **\"learning rate\"**. \n",
    "\n",
    "This page helps to explain the dangers of learning rates that are too large and too small: https://www.jeremyjordan.me/nn-learning-rate/.\n",
    "\n",
    "![learning_rate](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The general algorithm looks like this:\n",
    "\n",
    "We'll make a guess, $\\vec{s}$, at where our loss function attains a minimum. If we're not happy with how close the value of the gradient there is to 0, then we'll make a new guess, and the new guess will be constructed as follows:\n",
    "\n",
    "$\\large\\vec{s}_{new} = \\vec{s}_{old} - \\alpha\\nabla f(\\vec{s}_{old})$,\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "In the one-dimensional case, we'll have:\n",
    "\n",
    "$\\large x_{new} = x_{old} - \\alpha\\frac{df}{dx}|_{x_{old}}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
